
==> Audit <==
|------------|--------------------------------|----------|------------------------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  |          User          | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|------------------------|---------|---------------------|---------------------|
| start      | --driver=docke                 | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 23 Sep 24 18:31 IST |                     |
| start      | --driver=docker                | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 23 Sep 24 18:32 IST | 23 Sep 24 18:37 IST |
| start      |                                | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 23 Sep 24 23:07 IST | 23 Sep 24 23:08 IST |
| start      |                                | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 20:23 IST | 24 Sep 24 20:24 IST |
| docker-env |                                | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 20:25 IST | 24 Sep 24 20:25 IST |
| docker-env |                                | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 20:41 IST | 24 Sep 24 20:41 IST |
| ip         |                                | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 22:05 IST | 24 Sep 24 22:05 IST |
| dashboard  |                                | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 22:08 IST |                     |
| ip         |                                | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 22:18 IST | 24 Sep 24 22:18 IST |
| service    | userapp-service --url -n       | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 22:22 IST |                     |
|            | user-app                       |          |                        |         |                     |                     |
| service    | userapp-service --url -n       | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 22:23 IST |                     |
|            | user-app                       |          |                        |         |                     |                     |
| service    | userapp-service --url -n       | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 22:25 IST |                     |
|            | user-app                       |          |                        |         |                     |                     |
| service    | userapp-service --url -n       | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 22:26 IST |                     |
|            | user-app                       |          |                        |         |                     |                     |
| service    | userapp-service --url -n       | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 22:31 IST |                     |
|            | user-app                       |          |                        |         |                     |                     |
| service    | userapp-service                | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 22:38 IST |                     |
| service    | userapp-service --url -n       | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 22:39 IST |                     |
|            | user-app                       |          |                        |         |                     |                     |
| docker-env |                                | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 22:41 IST | 24 Sep 24 22:41 IST |
| service    | userapp-service --url -n       | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 22:41 IST |                     |
|            | user-app                       |          |                        |         |                     |                     |
| service    | userapp-service --url -n       | minikube | DESKTOP-4JUMADC\LENOVO | v1.34.0 | 24 Sep 24 22:44 IST |                     |
|            | user-app                       |          |                        |         |                     |                     |
|------------|--------------------------------|----------|------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/09/24 20:23:37
Running on machine: DESKTOP-4JUMADC
Binary: Built with gc go1.22.5 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0924 20:23:37.154212     844 out.go:345] Setting OutFile to fd 88 ...
I0924 20:23:37.155976     844 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0924 20:23:37.155976     844 out.go:358] Setting ErrFile to fd 92...
I0924 20:23:37.155976     844 out.go:392] TERM=,COLORTERM=, which probably does not support color
W0924 20:23:37.175665     844 root.go:314] Error reading config file at C:\Users\LENOVO\.minikube\config\config.json: open C:\Users\LENOVO\.minikube\config\config.json: The system cannot find the file specified.
I0924 20:23:37.198120     844 out.go:352] Setting JSON to false
I0924 20:23:37.203249     844 start.go:129] hostinfo: {"hostname":"DESKTOP-4JUMADC","uptime":97289,"bootTime":1727092327,"procs":246,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.4894 Build 19045.4894","kernelVersion":"10.0.19045.4894 Build 19045.4894","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"f8343a38-07eb-421b-bf3d-af42413fb92e"}
W0924 20:23:37.203249     844 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0924 20:23:37.206992     844 out.go:177] * minikube v1.34.0 on Microsoft Windows 10 Pro 10.0.19045.4894 Build 19045.4894
I0924 20:23:37.210020     844 notify.go:220] Checking for updates...
I0924 20:23:37.227032     844 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0924 20:23:37.230561     844 driver.go:394] Setting default libvirt URI to qemu:///system
W0924 20:23:37.249024     844 notify.go:59] Error getting json from minikube version url: error with http GET for endpoint https://storage.googleapis.com/minikube/releases-v2.json: Get "https://storage.googleapis.com/minikube/releases-v2.json": dial tcp: lookup storage.googleapis.com: no such host
I0924 20:23:37.866108     844 docker.go:123] docker version: linux-27.2.0:Docker Desktop 4.34.2 (167172)
I0924 20:23:37.874316     844 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0924 20:23:40.160994     844 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.2865722s)
I0924 20:23:40.166864     844 info.go:266] docker info: {ID:63f3d1ef-1979-4b18-82ca-acb6b3517102 Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:53 OomKillDisable:true NGoroutines:72 SystemTime:2024-09-24 14:53:40.130000201 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8246038528 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.2.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8fc6bcff51318944179630522a095cc9dbf9f353 Expected:8fc6bcff51318944179630522a095cc9dbf9f353} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.2-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.13.0]] Warnings:<nil>}}
I0924 20:23:40.169412     844 out.go:177] * Using the docker driver based on existing profile
I0924 20:23:40.171975     844 start.go:297] selected driver: docker
I0924 20:23:40.171975     844 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\LENOVO:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0924 20:23:40.172581     844 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0924 20:23:40.198461     844 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0924 20:23:40.411100     844 info.go:266] docker info: {ID:63f3d1ef-1979-4b18-82ca-acb6b3517102 Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:53 OomKillDisable:true NGoroutines:72 SystemTime:2024-09-24 14:53:40.397796412 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8246038528 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.2.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8fc6bcff51318944179630522a095cc9dbf9f353 Expected:8fc6bcff51318944179630522a095cc9dbf9f353} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.2-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.13.0]] Warnings:<nil>}}
I0924 20:23:40.457509     844 cni.go:84] Creating CNI manager for ""
I0924 20:23:40.458079     844 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0924 20:23:40.458741     844 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\LENOVO:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0924 20:23:40.459827     844 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0924 20:23:40.461586     844 cache.go:121] Beginning downloading kic base image for docker with docker
I0924 20:23:40.462095     844 out.go:177] * Pulling base image v0.0.45 ...
I0924 20:23:40.462768     844 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0924 20:23:40.462768     844 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I0924 20:23:40.463304     844 preload.go:146] Found local preload: C:\Users\LENOVO\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I0924 20:23:40.463304     844 cache.go:56] Caching tarball of preloaded images
I0924 20:23:40.463993     844 preload.go:172] Found C:\Users\LENOVO\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0924 20:23:40.463993     844 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I0924 20:23:40.463993     844 profile.go:143] Saving config to C:\Users\LENOVO\.minikube\profiles\minikube\config.json ...
W0924 20:23:40.536620     844 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I0924 20:23:40.536620     844 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I0924 20:23:40.538206     844 localpath.go:151] windows sanitize: C:\Users\LENOVO\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\LENOVO\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0924 20:23:40.538399     844 localpath.go:151] windows sanitize: C:\Users\LENOVO\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\LENOVO\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0924 20:23:40.539156     844 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I0924 20:23:40.541069     844 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I0924 20:23:40.541069     844 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I0924 20:23:40.541275     844 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I0924 20:23:40.541275     844 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I0924 20:23:40.541275     844 localpath.go:151] windows sanitize: C:\Users\LENOVO\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\LENOVO\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
W0924 20:23:41.199905     844 image.go:281] failed to pull image digest (expected if offline): Error response from daemon: Get "https://gcr.io/v2/": dialing gcr.io:443 container via direct connection because static system has no HTTPS proxy: connecting to gcr.io:443: dial tcp: lookup gcr.io: no such host
: exit status 1
I0924 20:23:41.199905     844 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I0924 20:23:41.199905     844 cache.go:194] Successfully downloaded all kic artifacts
I0924 20:23:41.201718     844 start.go:360] acquireMachinesLock for minikube: {Name:mk1f4826b908553e9bc66fb08644c23b5c53aa71 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0924 20:23:41.201718     844 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0924 20:23:41.201718     844 start.go:96] Skipping create...Using existing machine configuration
I0924 20:23:41.201718     844 fix.go:54] fixHost starting: 
I0924 20:23:41.213244     844 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0924 20:23:41.256426     844 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0924 20:23:41.256426     844 fix.go:138] unexpected machine state, will restart: <nil>
I0924 20:23:41.256955     844 out.go:177] * Restarting existing docker container for "minikube" ...
I0924 20:23:41.263755     844 cli_runner.go:164] Run: docker start minikube
I0924 20:23:42.004098     844 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0924 20:23:42.077617     844 kic.go:430] container "minikube" state is running.
I0924 20:23:42.096136     844 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0924 20:23:42.179019     844 profile.go:143] Saving config to C:\Users\LENOVO\.minikube\profiles\minikube\config.json ...
I0924 20:23:42.182471     844 machine.go:93] provisionDockerMachine start ...
I0924 20:23:42.197191     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0924 20:23:42.282422     844 main.go:141] libmachine: Using SSH client type: native
I0924 20:23:42.292064     844 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xdbc9c0] 0xdbf5a0 <nil>  [] 0s} 127.0.0.1 51281 <nil> <nil>}
I0924 20:23:42.292064     844 main.go:141] libmachine: About to run SSH command:
hostname
I0924 20:23:42.294090     844 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0924 20:23:45.544489     844 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0924 20:23:45.545751     844 ubuntu.go:169] provisioning hostname "minikube"
I0924 20:23:45.566762     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0924 20:23:45.622171     844 main.go:141] libmachine: Using SSH client type: native
I0924 20:23:45.622649     844 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xdbc9c0] 0xdbf5a0 <nil>  [] 0s} 127.0.0.1 51281 <nil> <nil>}
I0924 20:23:45.622649     844 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0924 20:23:45.937449     844 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0924 20:23:45.951306     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0924 20:23:46.001338     844 main.go:141] libmachine: Using SSH client type: native
I0924 20:23:46.001338     844 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xdbc9c0] 0xdbf5a0 <nil>  [] 0s} 127.0.0.1 51281 <nil> <nil>}
I0924 20:23:46.001338     844 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0924 20:23:46.205596     844 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0924 20:23:46.205596     844 ubuntu.go:175] set auth options {CertDir:C:\Users\LENOVO\.minikube CaCertPath:C:\Users\LENOVO\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\LENOVO\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\LENOVO\.minikube\machines\server.pem ServerKeyPath:C:\Users\LENOVO\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\LENOVO\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\LENOVO\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\LENOVO\.minikube}
I0924 20:23:46.205596     844 ubuntu.go:177] setting up certificates
I0924 20:23:46.205596     844 provision.go:84] configureAuth start
I0924 20:23:46.227438     844 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0924 20:23:46.276513     844 provision.go:143] copyHostCerts
I0924 20:23:46.289101     844 exec_runner.go:144] found C:\Users\LENOVO\.minikube/ca.pem, removing ...
I0924 20:23:46.289640     844 exec_runner.go:203] rm: C:\Users\LENOVO\.minikube\ca.pem
I0924 20:23:46.289849     844 exec_runner.go:151] cp: C:\Users\LENOVO\.minikube\certs\ca.pem --> C:\Users\LENOVO\.minikube/ca.pem (1078 bytes)
I0924 20:23:46.302002     844 exec_runner.go:144] found C:\Users\LENOVO\.minikube/cert.pem, removing ...
I0924 20:23:46.302002     844 exec_runner.go:203] rm: C:\Users\LENOVO\.minikube\cert.pem
I0924 20:23:46.303013     844 exec_runner.go:151] cp: C:\Users\LENOVO\.minikube\certs\cert.pem --> C:\Users\LENOVO\.minikube/cert.pem (1119 bytes)
I0924 20:23:46.320657     844 exec_runner.go:144] found C:\Users\LENOVO\.minikube/key.pem, removing ...
I0924 20:23:46.320657     844 exec_runner.go:203] rm: C:\Users\LENOVO\.minikube\key.pem
I0924 20:23:46.321299     844 exec_runner.go:151] cp: C:\Users\LENOVO\.minikube\certs\key.pem --> C:\Users\LENOVO\.minikube/key.pem (1679 bytes)
I0924 20:23:46.322526     844 provision.go:117] generating server cert: C:\Users\LENOVO\.minikube\machines\server.pem ca-key=C:\Users\LENOVO\.minikube\certs\ca.pem private-key=C:\Users\LENOVO\.minikube\certs\ca-key.pem org=LENOVO.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0924 20:23:46.463688     844 provision.go:177] copyRemoteCerts
I0924 20:23:46.486365     844 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0924 20:23:46.497149     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0924 20:23:46.545708     844 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51281 SSHKeyPath:C:\Users\LENOVO\.minikube\machines\minikube\id_rsa Username:docker}
I0924 20:23:46.693764     844 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0924 20:23:46.748509     844 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\machines\server.pem --> /etc/docker/server.pem (1180 bytes)
I0924 20:23:46.796734     844 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0924 20:23:46.822256     844 provision.go:87] duration metric: took 614.5421ms to configureAuth
I0924 20:23:46.822256     844 ubuntu.go:193] setting minikube options for container-runtime
I0924 20:23:46.822374     844 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0924 20:23:46.827338     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0924 20:23:46.874061     844 main.go:141] libmachine: Using SSH client type: native
I0924 20:23:46.874061     844 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xdbc9c0] 0xdbf5a0 <nil>  [] 0s} 127.0.0.1 51281 <nil> <nil>}
I0924 20:23:46.874061     844 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0924 20:23:47.074070     844 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0924 20:23:47.074202     844 ubuntu.go:71] root file system type: overlay
I0924 20:23:47.074677     844 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0924 20:23:47.095051     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0924 20:23:47.153506     844 main.go:141] libmachine: Using SSH client type: native
I0924 20:23:47.155543     844 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xdbc9c0] 0xdbf5a0 <nil>  [] 0s} 127.0.0.1 51281 <nil> <nil>}
I0924 20:23:47.155543     844 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0924 20:23:47.401578     844 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0924 20:23:47.415531     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0924 20:23:47.496104     844 main.go:141] libmachine: Using SSH client type: native
I0924 20:23:47.496891     844 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xdbc9c0] 0xdbf5a0 <nil>  [] 0s} 127.0.0.1 51281 <nil> <nil>}
I0924 20:23:47.496980     844 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0924 20:23:47.719126     844 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0924 20:23:47.719126     844 machine.go:96] duration metric: took 5.5366549s to provisionDockerMachine
I0924 20:23:47.719126     844 start.go:293] postStartSetup for "minikube" (driver="docker")
I0924 20:23:47.719126     844 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0924 20:23:47.745292     844 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0924 20:23:47.757212     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0924 20:23:47.816490     844 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51281 SSHKeyPath:C:\Users\LENOVO\.minikube\machines\minikube\id_rsa Username:docker}
I0924 20:23:47.976331     844 ssh_runner.go:195] Run: cat /etc/os-release
I0924 20:23:47.986311     844 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0924 20:23:47.986311     844 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0924 20:23:47.986311     844 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0924 20:23:47.986311     844 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0924 20:23:47.986311     844 filesync.go:126] Scanning C:\Users\LENOVO\.minikube\addons for local assets ...
I0924 20:23:47.986886     844 filesync.go:126] Scanning C:\Users\LENOVO\.minikube\files for local assets ...
I0924 20:23:47.986886     844 start.go:296] duration metric: took 267.7601ms for postStartSetup
I0924 20:23:48.004246     844 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0924 20:23:48.014232     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0924 20:23:48.102539     844 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51281 SSHKeyPath:C:\Users\LENOVO\.minikube\machines\minikube\id_rsa Username:docker}
I0924 20:23:48.260250     844 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0924 20:23:48.271987     844 fix.go:56] duration metric: took 7.0702697s for fixHost
I0924 20:23:48.271987     844 start.go:83] releasing machines lock for "minikube", held for 7.0702697s
I0924 20:23:48.280409     844 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0924 20:23:48.327595     844 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0924 20:23:48.334172     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0924 20:23:48.334172     844 ssh_runner.go:195] Run: cat /version.json
I0924 20:23:48.340578     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0924 20:23:48.382781     844 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51281 SSHKeyPath:C:\Users\LENOVO\.minikube\machines\minikube\id_rsa Username:docker}
I0924 20:23:48.387824     844 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51281 SSHKeyPath:C:\Users\LENOVO\.minikube\machines\minikube\id_rsa Username:docker}
W0924 20:23:48.519365     844 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0924 20:23:48.589036     844 ssh_runner.go:195] Run: systemctl --version
W0924 20:23:48.632939     844 out.go:270] ! Failing to connect to https://registry.k8s.io/ from both inside the minikube container and host machine
W0924 20:23:48.633678     844 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0924 20:23:48.669209     844 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0924 20:23:48.706172     844 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0924 20:23:48.737856     844 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0924 20:23:48.761667     844 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0924 20:23:48.789033     844 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0924 20:23:48.789033     844 start.go:495] detecting cgroup driver to use...
I0924 20:23:48.789106     844 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0924 20:23:48.790614     844 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0924 20:23:48.860262     844 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0924 20:23:48.914147     844 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0924 20:23:48.932693     844 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0924 20:23:48.940744     844 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0924 20:23:48.959573     844 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0924 20:23:48.977989     844 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0924 20:23:48.997753     844 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0924 20:23:49.016690     844 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0924 20:23:49.036565     844 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0924 20:23:49.055998     844 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0924 20:23:49.075268     844 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0924 20:23:49.094162     844 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0924 20:23:49.114543     844 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0924 20:23:49.132593     844 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0924 20:23:49.363572     844 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0924 20:23:49.624711     844 start.go:495] detecting cgroup driver to use...
I0924 20:23:49.624711     844 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0924 20:23:49.639040     844 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0924 20:23:49.655004     844 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0924 20:23:49.665010     844 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0924 20:23:49.681939     844 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0924 20:23:49.712251     844 ssh_runner.go:195] Run: which cri-dockerd
I0924 20:23:49.733550     844 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0924 20:23:49.745795     844 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0924 20:23:49.776526     844 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0924 20:23:49.978224     844 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0924 20:23:50.382939     844 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0924 20:23:50.382939     844 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0924 20:23:50.466823     844 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0924 20:23:50.702177     844 ssh_runner.go:195] Run: sudo systemctl restart docker
I0924 20:23:51.616263     844 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0924 20:23:51.642989     844 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0924 20:23:51.664432     844 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0924 20:23:51.684496     844 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0924 20:23:51.839735     844 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0924 20:23:52.052430     844 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0924 20:23:52.205872     844 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0924 20:23:52.237083     844 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0924 20:23:52.268763     844 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0924 20:23:52.399643     844 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0924 20:23:53.345222     844 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0924 20:23:53.356644     844 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0924 20:23:53.362988     844 start.go:563] Will wait 60s for crictl version
I0924 20:23:53.371723     844 ssh_runner.go:195] Run: which crictl
I0924 20:23:53.385115     844 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0924 20:23:53.761701     844 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I0924 20:23:53.769985     844 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0924 20:23:53.987959     844 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0924 20:23:54.014714     844 out.go:235] * Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I0924 20:23:54.020860     844 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0924 20:23:54.234033     844 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0924 20:23:54.247059     844 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0924 20:23:54.252494     844 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0924 20:23:54.272647     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0924 20:23:54.318152     844 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\LENOVO:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0924 20:23:54.318152     844 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0924 20:23:54.328686     844 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0924 20:23:54.355243     844 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0924 20:23:54.355243     844 docker.go:615] Images already preloaded, skipping extraction
I0924 20:23:54.360093     844 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0924 20:23:54.379619     844 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0924 20:23:54.379619     844 cache_images.go:84] Images are preloaded, skipping loading
I0924 20:23:54.379619     844 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I0924 20:23:54.380206     844 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0924 20:23:54.385490     844 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0924 20:23:54.972004     844 cni.go:84] Creating CNI manager for ""
I0924 20:23:54.972004     844 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0924 20:23:54.972131     844 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0924 20:23:54.972194     844 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0924 20:23:54.972249     844 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0924 20:23:54.994596     844 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I0924 20:23:55.022063     844 binaries.go:44] Found k8s binaries, skipping transfer
I0924 20:23:55.036489     844 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0924 20:23:55.050685     844 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0924 20:23:55.074175     844 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0924 20:23:55.095875     844 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0924 20:23:55.129428     844 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0924 20:23:55.135765     844 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0924 20:23:55.161632     844 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0924 20:23:55.363095     844 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0924 20:23:55.380249     844 certs.go:68] Setting up C:\Users\LENOVO\.minikube\profiles\minikube for IP: 192.168.49.2
I0924 20:23:55.380249     844 certs.go:194] generating shared ca certs ...
I0924 20:23:55.380249     844 certs.go:226] acquiring lock for ca certs: {Name:mke9df2fc6a4f59b06fbb05c903dcdb65dec22b7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0924 20:23:55.388291     844 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\LENOVO\.minikube\ca.key
I0924 20:23:55.406765     844 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\LENOVO\.minikube\proxy-client-ca.key
I0924 20:23:55.407451     844 certs.go:256] generating profile certs ...
I0924 20:23:55.408783     844 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\LENOVO\.minikube\profiles\minikube\client.key
I0924 20:23:55.439950     844 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\LENOVO\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0924 20:23:55.465974     844 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\LENOVO\.minikube\profiles\minikube\proxy-client.key
I0924 20:23:55.470547     844 certs.go:484] found cert: C:\Users\LENOVO\.minikube\certs\ca-key.pem (1679 bytes)
I0924 20:23:55.471140     844 certs.go:484] found cert: C:\Users\LENOVO\.minikube\certs\ca.pem (1078 bytes)
I0924 20:23:55.471393     844 certs.go:484] found cert: C:\Users\LENOVO\.minikube\certs\cert.pem (1119 bytes)
I0924 20:23:55.471393     844 certs.go:484] found cert: C:\Users\LENOVO\.minikube\certs\key.pem (1679 bytes)
I0924 20:23:55.479676     844 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0924 20:23:55.523928     844 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0924 20:23:55.564949     844 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0924 20:23:55.589846     844 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0924 20:23:55.616010     844 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0924 20:23:55.642664     844 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0924 20:23:55.669773     844 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0924 20:23:55.695541     844 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0924 20:23:55.720531     844 ssh_runner.go:362] scp C:\Users\LENOVO\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0924 20:23:55.746146     844 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0924 20:23:55.773921     844 ssh_runner.go:195] Run: openssl version
I0924 20:23:55.796171     844 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0924 20:23:55.818755     844 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0924 20:23:55.824420     844 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep 23 13:06 /usr/share/ca-certificates/minikubeCA.pem
I0924 20:23:55.832503     844 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0924 20:23:55.850853     844 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0924 20:23:55.872872     844 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0924 20:23:55.886153     844 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0924 20:23:55.905051     844 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0924 20:23:55.924881     844 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0924 20:23:55.941571     844 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0924 20:23:55.970930     844 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0924 20:23:56.005442     844 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0924 20:23:56.018460     844 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\LENOVO:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0924 20:23:56.027692     844 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0924 20:23:56.455412     844 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0924 20:23:56.612843     844 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0924 20:23:56.613727     844 kubeadm.go:593] restartPrimaryControlPlane start ...
I0924 20:23:56.663526     844 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0924 20:23:56.814597     844 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0924 20:23:56.843785     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0924 20:23:56.990996     844 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:53696"
I0924 20:23:56.991080     844 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:53696, want: 127.0.0.1:51285
I0924 20:23:56.992397     844 kubeconfig.go:62] C:\Users\LENOVO\.kube\config needs updating (will repair): [kubeconfig needs server address update]
I0924 20:23:56.993652     844 lock.go:35] WriteFile acquiring C:\Users\LENOVO\.kube\config: {Name:mkbb0d5237f4fb5206829a6920ada974ed50112f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0924 20:23:57.062204     844 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0924 20:23:57.132134     844 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0924 20:23:57.132191     844 kubeadm.go:597] duration metric: took 518.4639ms to restartPrimaryControlPlane
I0924 20:23:57.132191     844 kubeadm.go:394] duration metric: took 1.1137311s to StartCluster
I0924 20:23:57.132191     844 settings.go:142] acquiring lock: {Name:mk7cc598f23abbc1dd8bb11a8c1e633e312702c8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0924 20:23:57.133323     844 settings.go:150] Updating kubeconfig:  C:\Users\LENOVO\.kube\config
I0924 20:23:57.136165     844 lock.go:35] WriteFile acquiring C:\Users\LENOVO\.kube\config: {Name:mkbb0d5237f4fb5206829a6920ada974ed50112f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0924 20:23:57.138119     844 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0924 20:23:57.139072     844 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0924 20:23:57.138119     844 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0924 20:23:57.139072     844 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0924 20:23:57.139072     844 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0924 20:23:57.139072     844 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0924 20:23:57.139072     844 addons.go:243] addon storage-provisioner should already be in state true
I0924 20:23:57.139655     844 out.go:177] * Verifying Kubernetes components...
I0924 20:23:57.140016     844 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0924 20:23:57.140016     844 host.go:66] Checking if "minikube" exists ...
I0924 20:23:57.163243     844 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0924 20:23:57.171961     844 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0924 20:23:57.171961     844 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0924 20:23:57.256411     844 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0924 20:23:57.257487     844 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0924 20:23:57.257487     844 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0924 20:23:57.258970     844 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0924 20:23:57.258970     844 addons.go:243] addon default-storageclass should already be in state true
I0924 20:23:57.258970     844 host.go:66] Checking if "minikube" exists ...
I0924 20:23:57.263315     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0924 20:23:57.271625     844 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0924 20:23:57.331530     844 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51281 SSHKeyPath:C:\Users\LENOVO\.minikube\machines\minikube\id_rsa Username:docker}
I0924 20:23:57.337663     844 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I0924 20:23:57.337663     844 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0924 20:23:57.343514     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0924 20:23:57.387218     844 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51281 SSHKeyPath:C:\Users\LENOVO\.minikube\machines\minikube\id_rsa Username:docker}
I0924 20:23:57.940321     844 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0924 20:23:58.344171     844 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0924 20:23:58.441275     844 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0924 20:23:58.637197     844 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0924 20:23:58.815103     844 api_server.go:52] waiting for apiserver process to appear ...
I0924 20:23:58.826314     844 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0924 20:24:01.405341     844 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (3.0611695s)
W0924 20:24:01.405341     844 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0924 20:24:01.405341     844 retry.go:31] will retry after 327.735716ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0924 20:24:01.409150     844 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.5828354s)
I0924 20:24:01.409150     844 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.9678743s)
W0924 20:24:01.409150     844 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0924 20:24:01.409150     844 retry.go:31] will retry after 167.124482ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0924 20:24:01.428450     844 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0924 20:24:01.480485     844 api_server.go:72] duration metric: took 4.3423656s to wait for apiserver process to appear ...
I0924 20:24:01.480620     844 api_server.go:88] waiting for apiserver healthz status ...
I0924 20:24:01.480715     844 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51285/healthz ...
I0924 20:24:01.490668     844 api_server.go:269] stopped: https://127.0.0.1:51285/healthz: Get "https://127.0.0.1:51285/healthz": EOF
I0924 20:24:01.627751     844 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0924 20:24:01.761156     844 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0924 20:24:01.777284     844 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0924 20:24:01.777284     844 retry.go:31] will retry after 499.790107ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0924 20:24:01.886914     844 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0924 20:24:01.886914     844 retry.go:31] will retry after 417.190115ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0924 20:24:01.995824     844 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51285/healthz ...
I0924 20:24:01.998386     844 api_server.go:269] stopped: https://127.0.0.1:51285/healthz: Get "https://127.0.0.1:51285/healthz": EOF
I0924 20:24:02.304322     844 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0924 20:24:02.321946     844 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0924 20:24:02.488999     844 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51285/healthz ...
I0924 20:24:07.500120     844 api_server.go:269] stopped: https://127.0.0.1:51285/healthz: Get "https://127.0.0.1:51285/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0924 20:24:07.500120     844 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51285/healthz ...
I0924 20:24:12.099174     844 api_server.go:279] https://127.0.0.1:51285/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0924 20:24:12.099174     844 api_server.go:103] status: https://127.0.0.1:51285/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0924 20:24:12.099174     844 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51285/healthz ...
I0924 20:24:12.439602     844 api_server.go:279] https://127.0.0.1:51285/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0924 20:24:12.439602     844 api_server.go:103] status: https://127.0.0.1:51285/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0924 20:24:12.498145     844 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51285/healthz ...
I0924 20:24:12.605349     844 api_server.go:279] https://127.0.0.1:51285/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0924 20:24:12.605349     844 api_server.go:103] status: https://127.0.0.1:51285/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0924 20:24:12.987403     844 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51285/healthz ...
I0924 20:24:13.002629     844 api_server.go:279] https://127.0.0.1:51285/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0924 20:24:13.002629     844 api_server.go:103] status: https://127.0.0.1:51285/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0924 20:24:13.489200     844 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51285/healthz ...
I0924 20:24:13.503609     844 api_server.go:279] https://127.0.0.1:51285/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0924 20:24:13.503609     844 api_server.go:103] status: https://127.0.0.1:51285/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0924 20:24:13.986769     844 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51285/healthz ...
I0924 20:24:14.103908     844 api_server.go:279] https://127.0.0.1:51285/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0924 20:24:14.104020     844 api_server.go:103] status: https://127.0.0.1:51285/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0924 20:24:14.483363     844 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51285/healthz ...
I0924 20:24:14.604327     844 api_server.go:279] https://127.0.0.1:51285/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0924 20:24:14.605258     844 api_server.go:103] status: https://127.0.0.1:51285/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0924 20:24:14.997047     844 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51285/healthz ...
I0924 20:24:15.010627     844 api_server.go:279] https://127.0.0.1:51285/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0924 20:24:15.010627     844 api_server.go:103] status: https://127.0.0.1:51285/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0924 20:24:15.498850     844 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51285/healthz ...
I0924 20:24:15.714948     844 api_server.go:279] https://127.0.0.1:51285/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0924 20:24:15.714948     844 api_server.go:103] status: https://127.0.0.1:51285/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0924 20:24:15.984270     844 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51285/healthz ...
I0924 20:24:16.103703     844 api_server.go:279] https://127.0.0.1:51285/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0924 20:24:16.103703     844 api_server.go:103] status: https://127.0.0.1:51285/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0924 20:24:16.482515     844 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51285/healthz ...
I0924 20:24:16.495938     844 api_server.go:279] https://127.0.0.1:51285/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0924 20:24:16.495938     844 api_server.go:103] status: https://127.0.0.1:51285/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0924 20:24:16.995162     844 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51285/healthz ...
I0924 20:24:17.002768     844 api_server.go:279] https://127.0.0.1:51285/healthz returned 200:
ok
I0924 20:24:17.026952     844 api_server.go:141] control plane version: v1.31.0
I0924 20:24:17.026952     844 api_server.go:131] duration metric: took 15.5463321s to wait for apiserver health ...
I0924 20:24:17.027958     844 system_pods.go:43] waiting for kube-system pods to appear ...
I0924 20:24:17.043228     844 system_pods.go:59] 7 kube-system pods found
I0924 20:24:17.043228     844 system_pods.go:61] "coredns-6f6b679f8f-nsk7f" [b7909ff7-3640-46cf-bc70-c503ebd9b760] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0924 20:24:17.043228     844 system_pods.go:61] "etcd-minikube" [eb109023-e9fe-4266-bfb1-a8021bd94342] Running
I0924 20:24:17.043228     844 system_pods.go:61] "kube-apiserver-minikube" [8fde9683-139d-492b-b157-abedda5db84b] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0924 20:24:17.043228     844 system_pods.go:61] "kube-controller-manager-minikube" [d7896e40-3c26-4427-8162-8145045e537f] Running
I0924 20:24:17.043228     844 system_pods.go:61] "kube-proxy-46cg4" [97b5709f-7c3a-4e44-b9cb-d8e1dc2fdd7f] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0924 20:24:17.043228     844 system_pods.go:61] "kube-scheduler-minikube" [fe9f2037-0399-475b-b754-861fc12b1c98] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0924 20:24:17.043228     844 system_pods.go:61] "storage-provisioner" [7f32009b-1897-49fe-a19e-483bb8ffe88f] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0924 20:24:17.043228     844 system_pods.go:74] duration metric: took 15.2696ms to wait for pod list to return data ...
I0924 20:24:17.043228     844 kubeadm.go:582] duration metric: took 19.9051081s to wait for: map[apiserver:true system_pods:true]
I0924 20:24:17.043228     844 node_conditions.go:102] verifying NodePressure condition ...
I0924 20:24:17.081434     844 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0924 20:24:17.081978     844 node_conditions.go:123] node cpu capacity is 8
I0924 20:24:17.082484     844 node_conditions.go:105] duration metric: took 39.2567ms to run NodePressure ...
I0924 20:24:17.082520     844 start.go:241] waiting for startup goroutines ...
I0924 20:24:17.187889     844 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (14.8832484s)
I0924 20:24:17.187889     844 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (14.8659429s)
I0924 20:24:17.216297     844 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I0924 20:24:17.218095     844 addons.go:510] duration metric: took 20.0801892s for enable addons: enabled=[storage-provisioner default-storageclass]
I0924 20:24:17.218095     844 start.go:246] waiting for cluster config update ...
I0924 20:24:17.218095     844 start.go:255] writing updated cluster config ...
I0924 20:24:17.245731     844 ssh_runner.go:195] Run: rm -f paused
I0924 20:24:17.329796     844 out.go:177] * kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A'
I0924 20:24:17.330796     844 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Sep 24 16:38:54 minikube cri-dockerd[1398]: time="2024-09-24T16:38:54Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: Status: Downloaded newer image for kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Sep 24 16:38:56 minikube dockerd[1102]: time="2024-09-24T16:38:56.227014641Z" level=warning msg="reference for unknown type: " digest="sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" remote="docker.io/kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Sep 24 16:39:08 minikube cri-dockerd[1398]: time="2024-09-24T16:39:08Z" level=info msg="Pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: 978be80e3ee3: Extracting [==========================>                        ]  10.32MB/19.74MB"
Sep 24 16:39:09 minikube cri-dockerd[1398]: time="2024-09-24T16:39:09Z" level=info msg="Stop pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: Status: Downloaded newer image for kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Sep 24 16:39:17 minikube dockerd[1102]: time="2024-09-24T16:39:17.612139552Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:39:17 minikube dockerd[1102]: time="2024-09-24T16:39:17.612298769Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 16:39:23 minikube dockerd[1102]: time="2024-09-24T16:39:23.069374104Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:39:23 minikube dockerd[1102]: time="2024-09-24T16:39:23.069619231Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 16:39:26 minikube dockerd[1102]: time="2024-09-24T16:39:26.827518609Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:39:26 minikube dockerd[1102]: time="2024-09-24T16:39:26.827803040Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 16:39:31 minikube dockerd[1102]: time="2024-09-24T16:39:31.743384629Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:39:31 minikube dockerd[1102]: time="2024-09-24T16:39:31.743885283Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 16:44:25 minikube dockerd[1102]: time="2024-09-24T16:44:25.836974093Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:44:25 minikube dockerd[1102]: time="2024-09-24T16:44:25.837344839Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 16:44:33 minikube dockerd[1102]: time="2024-09-24T16:44:33.605925728Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:44:33 minikube dockerd[1102]: time="2024-09-24T16:44:33.606250966Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 16:44:40 minikube dockerd[1102]: time="2024-09-24T16:44:40.272178828Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:44:40 minikube dockerd[1102]: time="2024-09-24T16:44:40.272802298Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 16:44:46 minikube dockerd[1102]: time="2024-09-24T16:44:46.622122348Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:44:46 minikube dockerd[1102]: time="2024-09-24T16:44:46.622505294Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 16:49:46 minikube dockerd[1102]: time="2024-09-24T16:49:46.035626887Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:49:46 minikube dockerd[1102]: time="2024-09-24T16:49:46.036374378Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 16:49:50 minikube dockerd[1102]: time="2024-09-24T16:49:50.892615969Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:49:50 minikube dockerd[1102]: time="2024-09-24T16:49:50.892923709Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 16:49:56 minikube dockerd[1102]: time="2024-09-24T16:49:56.109256037Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:49:56 minikube dockerd[1102]: time="2024-09-24T16:49:56.109441661Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 16:50:02 minikube dockerd[1102]: time="2024-09-24T16:50:02.582467277Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:50:02 minikube dockerd[1102]: time="2024-09-24T16:50:02.583026349Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 16:55:03 minikube dockerd[1102]: time="2024-09-24T16:55:03.940923027Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:55:03 minikube dockerd[1102]: time="2024-09-24T16:55:03.941152355Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 16:55:09 minikube dockerd[1102]: time="2024-09-24T16:55:09.612922845Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:55:09 minikube dockerd[1102]: time="2024-09-24T16:55:09.613128970Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 16:55:15 minikube dockerd[1102]: time="2024-09-24T16:55:15.151246820Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:55:15 minikube dockerd[1102]: time="2024-09-24T16:55:15.151341331Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 16:55:20 minikube dockerd[1102]: time="2024-09-24T16:55:20.069087257Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 16:55:20 minikube dockerd[1102]: time="2024-09-24T16:55:20.069268680Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 17:00:16 minikube dockerd[1102]: time="2024-09-24T17:00:16.959975876Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 17:00:16 minikube dockerd[1102]: time="2024-09-24T17:00:16.960123094Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 17:00:23 minikube dockerd[1102]: time="2024-09-24T17:00:23.367276794Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 17:00:23 minikube dockerd[1102]: time="2024-09-24T17:00:23.367460917Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 17:00:28 minikube dockerd[1102]: time="2024-09-24T17:00:28.131617417Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 17:00:28 minikube dockerd[1102]: time="2024-09-24T17:00:28.131796339Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 17:00:32 minikube dockerd[1102]: time="2024-09-24T17:00:32.587671111Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 17:00:32 minikube dockerd[1102]: time="2024-09-24T17:00:32.587751421Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 17:05:31 minikube dockerd[1102]: time="2024-09-24T17:05:31.284426378Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 17:05:31 minikube dockerd[1102]: time="2024-09-24T17:05:31.284553893Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 17:05:37 minikube dockerd[1102]: time="2024-09-24T17:05:37.330966999Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 17:05:37 minikube dockerd[1102]: time="2024-09-24T17:05:37.331266333Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 17:05:44 minikube dockerd[1102]: time="2024-09-24T17:05:44.696602917Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 17:05:44 minikube dockerd[1102]: time="2024-09-24T17:05:44.696899754Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 17:05:50 minikube dockerd[1102]: time="2024-09-24T17:05:50.549132762Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 17:05:50 minikube dockerd[1102]: time="2024-09-24T17:05:50.549347489Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 17:10:49 minikube dockerd[1102]: time="2024-09-24T17:10:49.975322160Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 17:10:49 minikube dockerd[1102]: time="2024-09-24T17:10:49.975494784Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 17:10:55 minikube dockerd[1102]: time="2024-09-24T17:10:55.655171815Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 17:10:55 minikube dockerd[1102]: time="2024-09-24T17:10:55.655674985Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 17:11:00 minikube dockerd[1102]: time="2024-09-24T17:11:00.902971289Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 17:11:00 minikube dockerd[1102]: time="2024-09-24T17:11:00.903166216Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 24 17:11:06 minikube dockerd[1102]: time="2024-09-24T17:11:06.427326308Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 24 17:11:06 minikube dockerd[1102]: time="2024-09-24T17:11:06.427555140Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
9240d79dd51a2       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   36 minutes ago      Running             dashboard-metrics-scraper   0                   34235a7c32596       dashboard-metrics-scraper-c5db448b4-jrnf7
edcdabd35646c       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93         36 minutes ago      Running             kubernetes-dashboard        0                   5a2e160082c4f       kubernetes-dashboard-695b96c756-hmjf7
6ef6a3e38d390       6e38f40d628db                                                                                          2 hours ago         Running             storage-provisioner         5                   26f287bdd2acf       storage-provisioner
f76e1cc6d7212       cbb01a7bd410d                                                                                          2 hours ago         Running             coredns                     2                   9f4fc11df6533       coredns-6f6b679f8f-nsk7f
b14f164675ee9       6e38f40d628db                                                                                          2 hours ago         Exited              storage-provisioner         4                   26f287bdd2acf       storage-provisioner
580fd29f40c37       ad83b2ca7b09e                                                                                          2 hours ago         Running             kube-proxy                  2                   4e8436d409670       kube-proxy-46cg4
7b54f29575480       604f5db92eaa8                                                                                          2 hours ago         Running             kube-apiserver              2                   11b623886f1bf       kube-apiserver-minikube
02dc67569dd7f       045733566833c                                                                                          2 hours ago         Running             kube-controller-manager     2                   a0c5ebaaa40d0       kube-controller-manager-minikube
bb58553ba9620       2e96e5913fc06                                                                                          2 hours ago         Running             etcd                        2                   d39188696cdb3       etcd-minikube
0ec7b547661bb       1766f54c897f0                                                                                          2 hours ago         Running             kube-scheduler              2                   82631ba514a2b       kube-scheduler-minikube
3dabfd846bf05       cbb01a7bd410d                                                                                          24 hours ago        Exited              coredns                     1                   d9b30b39a6610       coredns-6f6b679f8f-nsk7f
c969465cc83a2       ad83b2ca7b09e                                                                                          24 hours ago        Exited              kube-proxy                  1                   09c4a777b4a87       kube-proxy-46cg4
2fbde4b385b5f       045733566833c                                                                                          24 hours ago        Exited              kube-controller-manager     1                   f3f8b2742b829       kube-controller-manager-minikube
407614a537f18       2e96e5913fc06                                                                                          24 hours ago        Exited              etcd                        1                   f460c3d5e6a98       etcd-minikube
bd2f4ea4c21e6       604f5db92eaa8                                                                                          24 hours ago        Exited              kube-apiserver              1                   3251c44ee1ffd       kube-apiserver-minikube
7a2e01a34d04e       1766f54c897f0                                                                                          24 hours ago        Exited              kube-scheduler              1                   9441ccafa52df       kube-scheduler-minikube


==> coredns [3dabfd846bf0] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:56622 - 39399 "HINFO IN 5175681233158874710.7099196695012280434. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.216261862s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[740891583]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (23-Sep-2024 17:38:16.894) (total time: 21044ms):
Trace[740891583]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21042ms (17:38:37.933)
Trace[740891583]: [21.044885736s] [21.044885736s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[119442834]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (23-Sep-2024 17:38:16.894) (total time: 21043ms):
Trace[119442834]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21042ms (17:38:37.933)
Trace[119442834]: [21.043645649s] [21.043645649s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[16896210]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (23-Sep-2024 17:38:16.894) (total time: 21043ms):
Trace[16896210]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21042ms (17:38:37.932)
Trace[16896210]: [21.04359734s] [21.04359734s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [f76e1cc6d721] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:43572 - 22109 "HINFO IN 8849636644386421562.1958030038272848392. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.227577251s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[843440576]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (24-Sep-2024 14:54:18.455) (total time: 21063ms):
Trace[843440576]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21060ms (14:54:39.511)
Trace[843440576]: [21.063610006s] [21.063610006s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1290371513]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (24-Sep-2024 14:54:18.455) (total time: 21064ms):
Trace[1290371513]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21060ms (14:54:39.512)
Trace[1290371513]: [21.064816281s] [21.064816281s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[892701364]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (24-Sep-2024 14:54:18.455) (total time: 21065ms):
Trace[892701364]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21061ms (14:54:39.512)
Trace[892701364]: [21.065377616s] [21.065377616s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_09_23T18_37_00_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 23 Sep 2024 13:06:57 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 24 Sep 2024 17:15:36 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 24 Sep 2024 17:15:04 +0000   Mon, 23 Sep 2024 13:06:54 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 24 Sep 2024 17:15:04 +0000   Mon, 23 Sep 2024 13:06:54 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 24 Sep 2024 17:15:04 +0000   Mon, 23 Sep 2024 13:06:54 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 24 Sep 2024 17:15:04 +0000   Mon, 23 Sep 2024 13:06:57 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8052772Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8052772Ki
  pods:               110
System Info:
  Machine ID:                 d9795bbd39404b9db110142e7aead04f
  System UUID:                d9795bbd39404b9db110142e7aead04f
  Boot ID:                    0dc309fc-97b2-4024-9af5-8542b59e07d2
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-6f6b679f8f-nsk7f                     100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     28h
  kube-system                 etcd-minikube                                100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         28h
  kube-system                 kube-apiserver-minikube                      250m (3%)     0 (0%)      0 (0%)           0 (0%)         28h
  kube-system                 kube-controller-manager-minikube             200m (2%)     0 (0%)      0 (0%)           0 (0%)         28h
  kube-system                 kube-proxy-46cg4                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         28h
  kube-system                 kube-scheduler-minikube                      100m (1%)     0 (0%)      0 (0%)           0 (0%)         28h
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         28h
  kubernetes-dashboard        dashboard-metrics-scraper-c5db448b4-jrnf7    0 (0%)        0 (0%)      0 (0%)           0 (0%)         37m
  kubernetes-dashboard        kubernetes-dashboard-695b96c756-hmjf7        0 (0%)        0 (0%)      0 (0%)           0 (0%)         37m
  user-app                    user-svc-deployment-5f8649bf9f-8vw85         0 (0%)        0 (0%)      0 (0%)           0 (0%)         43m
  user-app                    user-svc-deployment-5f8649bf9f-lws2g         0 (0%)        0 (0%)      0 (0%)           0 (0%)         43m
  user-app                    user-svc-deployment-5f8649bf9f-rvvt7         0 (0%)        0 (0%)      0 (0%)           0 (0%)         43m
  user-app                    user-svc-deployment-5f8649bf9f-vwqvh         0 (0%)        0 (0%)      0 (0%)           0 (0%)         43m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[Sep24 15:52] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] TAA CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/tsx_async_abort.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000000]  #2 #3 #4 #5 #6 #7
[  +0.060880] PCI: Fatal: No config space access function found
[  +0.069424] PCI: System does not support PCI
[  +0.143804] kvm: no hardware support
[  +0.000006] kvm: no hardware support
[  +2.444179] FS-Cache: Duplicate cookie detected
[  +0.000548] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000548] FS-Cache: O-cookie d=0000000041c67e97{9P.session} n=0000000099699575
[  +0.000691] FS-Cache: O-key=[10] '34323934393337353639'
[  +0.000466] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.000618] FS-Cache: N-cookie d=0000000041c67e97{9P.session} n=00000000398ed9f5
[  +0.001781] FS-Cache: N-key=[10] '34323934393337353639'
[  +0.004581] FS-Cache: Duplicate cookie detected
[  +0.009904] FS-Cache: O-cookie c=00000006 [p=00000002 fl=222 nc=0 na=1]
[  +0.000873] FS-Cache: O-cookie d=0000000041c67e97{9P.session} n=00000000f345a5ae
[  +0.001158] FS-Cache: O-key=[10] '34323934393337353730'
[  +0.000911] FS-Cache: N-cookie c=00000007 [p=00000002 fl=2 nc=0 na=1]
[  +0.002686] FS-Cache: N-cookie d=0000000041c67e97{9P.session} n=000000004165e69f
[  +0.001241] FS-Cache: N-key=[10] '34323934393337353730'
[  +1.778378] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000006]  failed 2
[  +0.035192] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +1.455535] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.015118] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000947] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001404] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001609] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +2.270377] netlink: 'init': attribute type 4 has an invalid length.
[Sep24 15:56] tmpfs: Unknown parameter 'noswap'


==> etcd [407614a537f1] <==
{"level":"info","ts":"2024-09-23T19:17:59.104341Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5277}
{"level":"info","ts":"2024-09-23T19:17:59.113391Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":5277,"took":"8.486634ms","hash":1120842059,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1331200,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-09-23T19:17:59.113581Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1120842059,"revision":5277,"compact-revision":5040}
{"level":"info","ts":"2024-09-23T19:22:59.090677Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5517}
{"level":"info","ts":"2024-09-23T19:22:59.100333Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":5517,"took":"8.997938ms","hash":1368630679,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1327104,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-09-23T19:22:59.100623Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1368630679,"revision":5517,"compact-revision":5277}
{"level":"info","ts":"2024-09-23T19:27:59.076598Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5754}
{"level":"info","ts":"2024-09-23T19:27:59.086367Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":5754,"took":"9.171224ms","hash":2347117424,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1343488,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-09-23T19:27:59.086562Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2347117424,"revision":5754,"compact-revision":5517}
{"level":"info","ts":"2024-09-23T19:32:59.073528Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5991}
{"level":"info","ts":"2024-09-23T19:32:59.084852Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":5991,"took":"10.712822ms","hash":3686287519,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1343488,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-09-23T19:32:59.085044Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3686287519,"revision":5991,"compact-revision":5754}
{"level":"info","ts":"2024-09-23T19:37:59.046553Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6230}
{"level":"info","ts":"2024-09-23T19:37:59.056061Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":6230,"took":"8.825697ms","hash":4245189378,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1318912,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-09-23T19:37:59.056258Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4245189378,"revision":6230,"compact-revision":5991}
{"level":"info","ts":"2024-09-23T19:42:59.037777Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6468}
{"level":"info","ts":"2024-09-23T19:42:59.048696Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":6468,"took":"9.692581ms","hash":892939924,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1327104,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-09-23T19:42:59.048937Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":892939924,"revision":6468,"compact-revision":6230}
{"level":"info","ts":"2024-09-23T19:47:59.048209Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6705}
{"level":"info","ts":"2024-09-23T19:47:59.059079Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":6705,"took":"10.150249ms","hash":46370341,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1339392,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-09-23T19:47:59.059333Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":46370341,"revision":6705,"compact-revision":6468}
{"level":"warn","ts":"2024-09-23T20:08:56.992556Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"398.103533ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-23T20:08:56.992639Z","caller":"traceutil/trace.go:171","msg":"trace[1814658723] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:7018; }","duration":"398.194347ms","start":"2024-09-23T20:08:56.594443Z","end":"2024-09-23T20:08:56.992624Z","steps":["trace[1814658723] 'agreement among raft nodes before linearized reading'  (duration: 398.073027ms)"],"step_count":1}
{"level":"info","ts":"2024-09-23T20:08:56.992703Z","caller":"traceutil/trace.go:171","msg":"trace[1703409835] transaction","detail":"{read_only:false; response_revision:7018; number_of_response:1; }","duration":"897.283137ms","start":"2024-09-23T19:49:32.123417Z","end":"2024-09-23T20:08:56.992687Z","steps":["trace[1703409835] 'process raft request'  (duration: 896.99449ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-23T20:08:56.992781Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-23T19:49:32.123402Z","time spent":"897.334945ms","remote":"127.0.0.1:46182","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:7015 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-09-23T20:08:56.992835Z","caller":"traceutil/trace.go:171","msg":"trace[13894914] linearizableReadLoop","detail":"{readStateIndex:8709; appliedIndex:8708; }","duration":"398.060226ms","start":"2024-09-23T20:08:56.594453Z","end":"2024-09-23T20:08:56.992500Z","steps":["trace[13894914] 'read index received'  (duration: 398.007017ms)","trace[13894914] 'applied index is now lower than readState.Index'  (duration: 52.709µs)"],"step_count":2}
{"level":"info","ts":"2024-09-23T20:12:23.033581Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6944}
{"level":"info","ts":"2024-09-23T20:12:23.038849Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":6944,"took":"4.850377ms","hash":1934789139,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1318912,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-09-23T20:12:23.038947Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1934789139,"revision":6944,"compact-revision":6705}
{"level":"info","ts":"2024-09-23T20:17:23.057116Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7181}
{"level":"info","ts":"2024-09-23T20:17:23.062916Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":7181,"took":"5.541216ms","hash":467952358,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1335296,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-09-23T20:17:23.063047Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":467952358,"revision":7181,"compact-revision":6944}
{"level":"info","ts":"2024-09-23T20:22:23.049904Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7419}
{"level":"info","ts":"2024-09-23T20:22:23.055176Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":7419,"took":"4.924112ms","hash":3161439069,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1363968,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-09-23T20:22:23.055274Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3161439069,"revision":7419,"compact-revision":7181}
{"level":"info","ts":"2024-09-23T20:27:23.054527Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7660}
{"level":"info","ts":"2024-09-23T20:27:23.058499Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":7660,"took":"3.688814ms","hash":2077614200,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1335296,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-09-23T20:27:23.058575Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2077614200,"revision":7660,"compact-revision":7419}
{"level":"info","ts":"2024-09-23T20:30:30.689491Z","caller":"etcdserver/server.go:1451","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":10001,"local-member-snapshot-index":0,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-09-23T20:30:30.694804Z","caller":"etcdserver/server.go:2471","msg":"saved snapshot","snapshot-index":10001}
{"level":"info","ts":"2024-09-23T20:30:30.694881Z","caller":"etcdserver/server.go:2501","msg":"compacted Raft logs","compact-index":5001}
{"level":"info","ts":"2024-09-23T20:32:23.062999Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7898}
{"level":"info","ts":"2024-09-23T20:32:23.071070Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":7898,"took":"7.594662ms","hash":2714857435,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1355776,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-09-23T20:32:23.071239Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2714857435,"revision":7898,"compact-revision":7660}
{"level":"info","ts":"2024-09-23T20:37:23.055885Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8137}
{"level":"info","ts":"2024-09-23T20:37:23.060344Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":8137,"took":"4.154459ms","hash":2940731482,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1363968,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-09-23T20:37:23.060420Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2940731482,"revision":8137,"compact-revision":7898}
{"level":"info","ts":"2024-09-23T20:42:23.060669Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8376}
{"level":"info","ts":"2024-09-23T20:42:23.064611Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":8376,"took":"3.697302ms","hash":2772927161,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1355776,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-09-23T20:42:23.064682Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2772927161,"revision":8376,"compact-revision":8137}
{"level":"info","ts":"2024-09-23T20:44:24.224870Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-09-23T20:44:24.224915Z","caller":"embed/etcd.go:377","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-09-23T20:44:24.224983Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-09-23T20:44:24.225003Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-09-23T20:44:24.225068Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-09-23T20:44:24.225128Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"info","ts":"2024-09-23T20:44:24.316875Z","caller":"etcdserver/server.go:1521","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-09-23T20:44:24.419157Z","caller":"embed/etcd.go:581","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-09-23T20:44:24.419374Z","caller":"embed/etcd.go:586","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-09-23T20:44:24.419406Z","caller":"embed/etcd.go:379","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [bb58553ba962] <==
{"level":"info","ts":"2024-09-24T15:24:04.324203Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9963}
{"level":"info","ts":"2024-09-24T15:24:04.334433Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":9963,"took":"9.629594ms","hash":2599604274,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1409024,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-09-24T15:24:04.334637Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2599604274,"revision":9963,"compact-revision":9725}
{"level":"warn","ts":"2024-09-24T16:29:48.690519Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"637.390106ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-24T16:29:48.690627Z","caller":"traceutil/trace.go:171","msg":"trace[424815654] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:10366; }","duration":"637.507924ms","start":"2024-09-24T15:27:34.024853Z","end":"2024-09-24T16:29:48.690607Z","steps":["trace[424815654] 'range keys from in-memory index tree'  (duration: 637.370504ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-24T16:29:48.690669Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"637.135369ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/\" range_end:\"/registry/persistentvolumes0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-24T16:29:48.690712Z","caller":"traceutil/trace.go:171","msg":"trace[1191423310] range","detail":"{range_begin:/registry/persistentvolumes/; range_end:/registry/persistentvolumes0; response_count:0; response_revision:10366; }","duration":"637.189577ms","start":"2024-09-24T15:27:34.025266Z","end":"2024-09-24T16:29:48.690701Z","steps":["trace[1191423310] 'count revisions from in-memory index tree'  (duration: 637.082161ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-24T16:29:48.690790Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-24T15:27:34.025220Z","time spent":"637.310495ms","remote":"127.0.0.1:37762","response type":"/etcdserverpb.KV/Range","request count":0,"request size":62,"response count":0,"response size":29,"request content":"key:\"/registry/persistentvolumes/\" range_end:\"/registry/persistentvolumes0\" count_only:true "}
{"level":"warn","ts":"2024-09-24T16:29:48.690788Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"631.546347ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-24T16:29:48.690953Z","caller":"traceutil/trace.go:171","msg":"trace[1570878625] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:10366; }","duration":"631.710671ms","start":"2024-09-24T15:27:34.030984Z","end":"2024-09-24T16:29:48.690940Z","steps":["trace[1570878625] 'range keys from in-memory index tree'  (duration: 631.448033ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-24T16:29:48.690994Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-24T15:27:34.030953Z","time spent":"631.783482ms","remote":"127.0.0.1:37620","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-09-24T16:31:18.321181Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10201}
{"level":"info","ts":"2024-09-24T16:31:18.324611Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":10201,"took":"3.167991ms","hash":3485199833,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1368064,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-09-24T16:31:18.324652Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3485199833,"revision":10201,"compact-revision":9963}
{"level":"info","ts":"2024-09-24T16:33:08.877251Z","caller":"traceutil/trace.go:171","msg":"trace[760605190] transaction","detail":"{read_only:false; response_revision:10622; number_of_response:1; }","duration":"100.286226ms","start":"2024-09-24T16:33:08.776934Z","end":"2024-09-24T16:33:08.877220Z","steps":["trace[760605190] 'process raft request'  (duration: 97.046421ms)"],"step_count":1}
{"level":"info","ts":"2024-09-24T16:36:18.308957Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10439}
{"level":"info","ts":"2024-09-24T16:36:18.318038Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":10439,"took":"8.273899ms","hash":909966896,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1802240,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-09-24T16:36:18.318228Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":909966896,"revision":10439,"compact-revision":10201}
{"level":"info","ts":"2024-09-24T16:38:13.941785Z","caller":"traceutil/trace.go:171","msg":"trace[1257441675] transaction","detail":"{read_only:false; response_revision:10995; number_of_response:1; }","duration":"190.21817ms","start":"2024-09-24T16:38:13.751474Z","end":"2024-09-24T16:38:13.941692Z","steps":["trace[1257441675] 'process raft request'  (duration: 19.858285ms)"],"step_count":1}
{"level":"info","ts":"2024-09-24T16:38:13.954109Z","caller":"traceutil/trace.go:171","msg":"trace[91017751] transaction","detail":"{read_only:false; response_revision:10997; number_of_response:1; }","duration":"192.467443ms","start":"2024-09-24T16:38:13.761519Z","end":"2024-09-24T16:38:13.953986Z","steps":["trace[91017751] 'process raft request'  (duration: 101.799042ms)","trace[91017751] 'compare'  (duration: 90.30184ms)"],"step_count":2}
{"level":"info","ts":"2024-09-24T16:38:13.955542Z","caller":"traceutil/trace.go:171","msg":"trace[1366616891] linearizableReadLoop","detail":"{readStateIndex:13641; appliedIndex:13640; }","duration":"112.799662ms","start":"2024-09-24T16:38:13.842237Z","end":"2024-09-24T16:38:13.955037Z","steps":["trace[1366616891] 'read index received'  (duration: 21.134296ms)","trace[1366616891] 'applied index is now lower than readState.Index'  (duration: 91.655264ms)"],"step_count":2}
{"level":"warn","ts":"2024-09-24T16:38:13.957309Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"115.029431ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kubernetes-dashboard/kubernetes-dashboard-695b96c756-hmjf7\" ","response":"range_response_count:1 size:2835"}
{"level":"info","ts":"2024-09-24T16:38:13.957582Z","caller":"traceutil/trace.go:171","msg":"trace[2070838794] range","detail":"{range_begin:/registry/pods/kubernetes-dashboard/kubernetes-dashboard-695b96c756-hmjf7; range_end:; response_count:1; response_revision:10997; }","duration":"115.339982ms","start":"2024-09-24T16:38:13.842184Z","end":"2024-09-24T16:38:13.957524Z","steps":["trace[2070838794] 'agreement among raft nodes before linearized reading'  (duration: 114.617663ms)"],"step_count":1}
{"level":"info","ts":"2024-09-24T16:38:14.270253Z","caller":"traceutil/trace.go:171","msg":"trace[2041764429] transaction","detail":"{read_only:false; response_revision:11006; number_of_response:1; }","duration":"104.964365ms","start":"2024-09-24T16:38:14.165123Z","end":"2024-09-24T16:38:14.270087Z","steps":["trace[2041764429] 'process raft request'  (duration: 103.84588ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-24T16:38:14.271119Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"106.433709ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/kubernetes-dashboard/kubernetes-dashboard-695b96c756.17f83b839e956eca\" ","response":"range_response_count:1 size:941"}
{"level":"info","ts":"2024-09-24T16:38:14.271291Z","caller":"traceutil/trace.go:171","msg":"trace[181803636] range","detail":"{range_begin:/registry/events/kubernetes-dashboard/kubernetes-dashboard-695b96c756.17f83b839e956eca; range_end:; response_count:1; response_revision:11008; }","duration":"106.622139ms","start":"2024-09-24T16:38:14.164620Z","end":"2024-09-24T16:38:14.271242Z","steps":["trace[181803636] 'agreement among raft nodes before linearized reading'  (duration: 106.074549ms)"],"step_count":1}
{"level":"info","ts":"2024-09-24T16:38:55.377842Z","caller":"traceutil/trace.go:171","msg":"trace[1543435507] linearizableReadLoop","detail":"{readStateIndex:13712; appliedIndex:13711; }","duration":"124.660379ms","start":"2024-09-24T16:38:55.253160Z","end":"2024-09-24T16:38:55.377820Z","steps":["trace[1543435507] 'read index received'  (duration: 124.627176ms)","trace[1543435507] 'applied index is now lower than readState.Index'  (duration: 32.103µs)"],"step_count":2}
{"level":"info","ts":"2024-09-24T16:38:55.378154Z","caller":"traceutil/trace.go:171","msg":"trace[975021506] transaction","detail":"{read_only:false; response_revision:11058; number_of_response:1; }","duration":"161.00821ms","start":"2024-09-24T16:38:55.217125Z","end":"2024-09-24T16:38:55.378133Z","steps":["trace[975021506] 'process raft request'  (duration: 160.492664ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-24T16:38:55.378166Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"124.980508ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/priorityclasses/\" range_end:\"/registry/priorityclasses0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-09-24T16:38:55.378227Z","caller":"traceutil/trace.go:171","msg":"trace[1098484554] range","detail":"{range_begin:/registry/priorityclasses/; range_end:/registry/priorityclasses0; response_count:0; response_revision:11058; }","duration":"125.063115ms","start":"2024-09-24T16:38:55.253148Z","end":"2024-09-24T16:38:55.378211Z","steps":["trace[1098484554] 'agreement among raft nodes before linearized reading'  (duration: 124.818593ms)"],"step_count":1}
{"level":"info","ts":"2024-09-24T16:41:09.850919Z","caller":"traceutil/trace.go:171","msg":"trace[1545922891] transaction","detail":"{read_only:false; response_revision:11189; number_of_response:1; }","duration":"101.963612ms","start":"2024-09-24T16:41:09.748879Z","end":"2024-09-24T16:41:09.850842Z","steps":["trace[1545922891] 'process raft request'  (duration: 101.244625ms)"],"step_count":1}
{"level":"info","ts":"2024-09-24T16:41:18.313968Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10864}
{"level":"info","ts":"2024-09-24T16:41:18.325283Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":10864,"took":"10.525501ms","hash":892434944,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":2301952,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-09-24T16:41:18.325582Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":892434944,"revision":10864,"compact-revision":10439}
{"level":"info","ts":"2024-09-24T16:42:05.025217Z","caller":"traceutil/trace.go:171","msg":"trace[2115420278] linearizableReadLoop","detail":"{readStateIndex:13926; appliedIndex:13925; }","duration":"101.566163ms","start":"2024-09-24T16:42:04.923528Z","end":"2024-09-24T16:42:05.025094Z","steps":["trace[2115420278] 'read index received'  (duration: 100.99451ms)","trace[2115420278] 'applied index is now lower than readState.Index'  (duration: 568.753µs)"],"step_count":2}
{"level":"info","ts":"2024-09-24T16:42:05.025761Z","caller":"traceutil/trace.go:171","msg":"trace[676675313] transaction","detail":"{read_only:false; response_revision:11233; number_of_response:1; }","duration":"105.387019ms","start":"2024-09-24T16:42:04.920320Z","end":"2024-09-24T16:42:05.025707Z","steps":["trace[676675313] 'process raft request'  (duration: 104.374925ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-24T16:42:05.026170Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.468547ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-24T16:42:05.026359Z","caller":"traceutil/trace.go:171","msg":"trace[287523501] range","detail":"{range_begin:/registry/replicasets; range_end:; response_count:0; response_revision:11233; }","duration":"102.800079ms","start":"2024-09-24T16:42:04.923516Z","end":"2024-09-24T16:42:05.026316Z","steps":["trace[287523501] 'agreement among raft nodes before linearized reading'  (duration: 102.391941ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-24T16:42:05.026494Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.378639ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/user-app/\" range_end:\"/registry/events/user-app0\" ","response":"range_response_count:32 size:25292"}
{"level":"info","ts":"2024-09-24T16:42:05.026796Z","caller":"traceutil/trace.go:171","msg":"trace[440672669] range","detail":"{range_begin:/registry/events/user-app/; range_end:/registry/events/user-app0; response_count:32; response_revision:11233; }","duration":"102.549555ms","start":"2024-09-24T16:42:04.924056Z","end":"2024-09-24T16:42:05.026605Z","steps":["trace[440672669] 'agreement among raft nodes before linearized reading'  (duration: 101.708276ms)"],"step_count":1}
{"level":"info","ts":"2024-09-24T16:46:18.303353Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11197}
{"level":"info","ts":"2024-09-24T16:46:18.315580Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":11197,"took":"11.527824ms","hash":2135157808,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":2080768,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-09-24T16:46:18.315958Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2135157808,"revision":11197,"compact-revision":10864}
{"level":"info","ts":"2024-09-24T16:50:17.770930Z","caller":"traceutil/trace.go:171","msg":"trace[1573631315] transaction","detail":"{read_only:false; response_revision:11646; number_of_response:1; }","duration":"106.794453ms","start":"2024-09-24T16:50:17.664078Z","end":"2024-09-24T16:50:17.770872Z","steps":["trace[1573631315] 'process raft request'  (duration: 106.256583ms)"],"step_count":1}
{"level":"info","ts":"2024-09-24T16:51:18.309752Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11447}
{"level":"info","ts":"2024-09-24T16:51:18.320264Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":11447,"took":"9.804052ms","hash":3958203890,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1781760,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-09-24T16:51:18.320521Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3958203890,"revision":11447,"compact-revision":11197}
{"level":"info","ts":"2024-09-24T16:56:18.310284Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11697}
{"level":"info","ts":"2024-09-24T16:56:18.320980Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":11697,"took":"9.682857ms","hash":3459069446,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1761280,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-09-24T16:56:18.321227Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3459069446,"revision":11697,"compact-revision":11447}
{"level":"info","ts":"2024-09-24T17:01:18.292446Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11946}
{"level":"info","ts":"2024-09-24T17:01:18.304656Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":11946,"took":"11.306356ms","hash":1063717309,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1753088,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-09-24T17:01:18.304920Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1063717309,"revision":11946,"compact-revision":11697}
{"level":"info","ts":"2024-09-24T17:06:18.263961Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12199}
{"level":"info","ts":"2024-09-24T17:06:18.268621Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":12199,"took":"4.331357ms","hash":3144438050,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1794048,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-09-24T17:06:18.268760Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3144438050,"revision":12199,"compact-revision":11946}
{"level":"info","ts":"2024-09-24T17:06:25.053703Z","caller":"traceutil/trace.go:171","msg":"trace[1204887460] transaction","detail":"{read_only:false; response_revision:12455; number_of_response:1; }","duration":"106.166303ms","start":"2024-09-24T17:06:24.947393Z","end":"2024-09-24T17:06:25.053559Z","steps":["trace[1204887460] 'process raft request'  (duration: 105.479315ms)"],"step_count":1}
{"level":"info","ts":"2024-09-24T17:11:18.251338Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12449}
{"level":"info","ts":"2024-09-24T17:11:18.302095Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":12449,"took":"50.343864ms","hash":1320488555,"current-db-size-bytes":2404352,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1638400,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-09-24T17:11:18.302317Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1320488555,"revision":12449,"compact-revision":12199}


==> kernel <==
 17:15:43 up  1:23,  0 users,  load average: 1.62, 0.97, 0.70
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [7b54f2957548] <==
I0924 14:54:11.944468       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I0924 14:54:11.944521       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0924 14:54:11.944662       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0924 14:54:11.944798       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0924 14:54:11.944834       1 aggregator.go:169] waiting for initial CRD sync...
I0924 14:54:11.946113       1 controller.go:78] Starting OpenAPI AggregationController
I0924 14:54:11.946373       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0924 14:54:11.946483       1 controller.go:142] Starting OpenAPI controller
I0924 14:54:11.946627       1 controller.go:90] Starting OpenAPI V3 controller
I0924 14:54:11.946827       1 naming_controller.go:294] Starting NamingConditionController
I0924 14:54:11.947003       1 establishing_controller.go:81] Starting EstablishingController
I0924 14:54:11.947152       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0924 14:54:11.947195       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0924 14:54:11.947228       1 crd_finalizer.go:269] Starting CRDFinalizer
I0924 14:54:11.977823       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0924 14:54:11.978618       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0924 14:54:11.944656       1 local_available_controller.go:156] Starting LocalAvailability controller
I0924 14:54:11.980660       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0924 14:54:12.094154       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0924 14:54:12.094241       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0924 14:54:12.281445       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0924 14:54:12.282219       1 shared_informer.go:320] Caches are synced for configmaps
I0924 14:54:12.286539       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0924 14:54:12.286839       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0924 14:54:12.287041       1 cache.go:39] Caches are synced for LocalAvailability controller
I0924 14:54:12.299788       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I0924 14:54:12.376779       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0924 14:54:12.377466       1 aggregator.go:171] initial CRD sync complete...
I0924 14:54:12.377480       1 autoregister_controller.go:144] Starting autoregister controller
I0924 14:54:12.377488       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0924 14:54:12.377496       1 cache.go:39] Caches are synced for autoregister controller
I0924 14:54:12.397386       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0924 14:54:12.397462       1 policy_source.go:224] refreshing policies
I0924 14:54:12.397532       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0924 14:54:12.397552       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0924 14:54:12.476053       1 shared_informer.go:320] Caches are synced for node_authorizer
I0924 14:54:12.477432       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0924 14:54:12.986630       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0924 14:54:18.923285       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0924 14:54:18.989500       1 controller.go:615] quota admission added evaluator for: endpoints
E0924 14:54:22.676487       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: dc3bfab7-1e68-40a7-9123-ab3e5f21acec, UID in object meta: "
E0924 16:30:03.563269       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0924 16:30:04.109205       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0924 16:31:46.784732       1 controller.go:615] quota admission added evaluator for: namespaces
I0924 16:31:46.800872       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0924 16:32:13.670991       1 controller.go:615] quota admission added evaluator for: deployments.apps
E0924 16:32:13.677610       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0924 16:32:13.698687       1 controller.go:615] quota admission added evaluator for: replicasets.apps
E0924 16:32:13.704530       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0924 16:32:33.728550       1 alloc.go:330] "allocated clusterIPs" service="user-app/userapp-service" clusterIPs={"IPv4":"10.102.45.129"}
E0924 16:32:33.732400       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0924 16:32:33.732779       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0924 16:38:13.581618       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0924 16:38:13.596260       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0924 16:38:14.667646       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/kubernetes-dashboard" clusterIPs={"IPv4":"10.100.73.52"}
I0924 16:38:14.941904       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/dashboard-metrics-scraper" clusterIPs={"IPv4":"10.97.227.52"}
E0924 16:39:23.480667       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"context canceled\"}: context canceled" logger="UnhandledError"
E0924 16:39:23.480809       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"context canceled\"}: context canceled" logger="UnhandledError"
E0924 16:39:23.481129       1 storage.go:479] "Unhandled Error" err="Address {10.244.0.9  0xc00428bd30 0xc0097b7340} isn't valid (context canceled)" logger="UnhandledError"
E0924 16:39:23.529108       1 storage.go:489] "Unhandled Error" err="Failed to find a valid address, skipping subset: &{[{10.244.0.9  0xc00428bd30 0xc0097b7340}] [] [{ 9090 TCP <nil>}]}" logger="UnhandledError"


==> kube-apiserver [bd2f4ea4c21e] <==
W0923 20:44:24.308837       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:24.308873       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:24.308839       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:24.308916       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.234084       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.234308       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.234129       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.234520       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.234588       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.234642       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.235042       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.306841       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.306953       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.306897       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.307149       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.307192       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.307898       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.307994       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308032       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308049       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308094       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308166       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308177       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308192       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308209       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308220       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308212       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308272       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308339       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308354       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308377       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308440       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308453       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308479       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308512       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308487       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308548       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308601       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308670       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308692       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308732       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308740       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308788       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308872       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.308914       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.309010       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.309043       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.309059       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.309014       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.309151       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.309160       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.309195       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.309154       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.309266       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.309286       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.309293       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.309337       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.309391       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.309427       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0923 20:44:25.309600       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [02dc67569dd7] <==
I0924 16:39:42.672927       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="100.911µs"
I0924 16:39:43.703699       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="178.919µs"
I0924 16:39:46.686836       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="175.52µs"
I0924 16:39:50.688115       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="203.623µs"
I0924 16:39:55.688804       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="420.147µs"
I0924 16:44:27.013797       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0924 16:44:37.653505       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="332.937µs"
I0924 16:44:47.669081       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="306.337µs"
I0924 16:44:48.709240       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="395.547µs"
I0924 16:44:53.678384       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="312.638µs"
I0924 16:44:57.652976       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="185.422µs"
I0924 16:45:02.665202       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="424.45µs"
I0924 16:45:07.706064       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="184.222µs"
I0924 16:45:12.647515       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="563.065µs"
I0924 16:49:32.806890       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0924 16:49:58.602461       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="69.709µs"
I0924 16:50:05.638795       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="334.141µs"
I0924 16:50:06.626815       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="184.223µs"
I0924 16:50:09.662141       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="238.929µs"
I0924 16:50:13.599131       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="121.015µs"
I0924 16:50:17.775762       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="182.824µs"
I0924 16:50:17.871928       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="175.823µs"
I0924 16:50:25.623482       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="193.226µs"
I0924 16:54:40.129181       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0924 16:55:16.589034       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="350.843µs"
I0924 16:55:24.568630       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="482.361µs"
I0924 16:55:27.583600       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="233.83µs"
I0924 16:55:30.617742       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="175.722µs"
I0924 16:55:33.587623       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="179.922µs"
I0924 16:55:37.575047       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="375.248µs"
I0924 16:55:38.572506       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="299.739µs"
I0924 16:55:46.545747       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="123.916µs"
I0924 16:59:45.664896       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0924 17:00:30.517642       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="109.513µs"
I0924 17:00:37.594333       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="321.539µs"
I0924 17:00:40.549988       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="157.019µs"
I0924 17:00:43.552991       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="262.531µs"
I0924 17:00:44.559181       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="406.648µs"
I0924 17:00:50.545340       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="379.748µs"
I0924 17:00:54.550482       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="327.841µs"
I0924 17:00:59.551643       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="377.248µs"
I0924 17:04:52.006359       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0924 17:05:45.546923       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="291.837µs"
I0924 17:05:50.506967       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="355.145µs"
I0924 17:05:58.499090       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="284.439µs"
I0924 17:06:00.497234       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="125.116µs"
I0924 17:06:02.476037       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="68.908µs"
I0924 17:06:03.545430       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="181.423µs"
I0924 17:06:12.497728       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="113.615µs"
I0924 17:06:16.496651       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="125.316µs"
I0924 17:09:58.640653       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0924 17:11:03.460997       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="390.254µs"
I0924 17:11:06.462676       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="368.951µs"
I0924 17:11:16.438392       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="267.238µs"
I0924 17:11:18.466679       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="354.951µs"
I0924 17:11:20.442464       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="81.811µs"
I0924 17:11:20.455153       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="57.108µs"
I0924 17:11:27.448926       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="143.022µs"
I0924 17:11:32.472655       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="user-app/user-svc-deployment-5f8649bf9f" duration="207.83µs"
I0924 17:15:04.872113       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [2fbde4b385b5] <==
I0923 17:38:15.049442       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0923 17:38:15.056439       1 shared_informer.go:320] Caches are synced for attach detach
I0923 17:38:15.057010       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0923 17:38:15.057036       1 shared_informer.go:320] Caches are synced for endpoint
I0923 17:38:15.057577       1 shared_informer.go:320] Caches are synced for persistent volume
I0923 17:38:15.057630       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0923 17:38:15.057941       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0923 17:38:15.058072       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0923 17:38:15.135888       1 shared_informer.go:320] Caches are synced for job
I0923 17:38:15.139192       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0923 17:38:15.139260       1 shared_informer.go:320] Caches are synced for disruption
I0923 17:38:15.139338       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0923 17:38:15.154581       1 shared_informer.go:320] Caches are synced for stateful set
I0923 17:38:15.236378       1 shared_informer.go:320] Caches are synced for deployment
I0923 17:38:15.236655       1 shared_informer.go:320] Caches are synced for cronjob
I0923 17:38:15.257947       1 shared_informer.go:320] Caches are synced for resource quota
I0923 17:38:15.339307       1 shared_informer.go:320] Caches are synced for resource quota
I0923 17:38:15.635933       1 shared_informer.go:320] Caches are synced for garbage collector
I0923 17:38:15.636988       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="497.468096ms"
I0923 17:38:15.638949       1 shared_informer.go:320] Caches are synced for garbage collector
I0923 17:38:15.638996       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0923 17:38:15.639949       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="321.248µs"
I0923 17:38:17.260290       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="62.809µs"
I0923 17:38:44.216538       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="22.055741ms"
I0923 17:38:44.216853       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="128.92µs"
I0923 17:43:09.232694       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 17:48:15.384355       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 17:53:22.069144       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 17:58:28.222375       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 18:03:34.489059       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 18:08:41.005578       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 18:13:46.228387       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 18:18:52.228089       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 18:23:58.269375       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 18:29:04.742201       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 18:34:10.682140       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 18:38:11.530575       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-mwdzg" approvedExpiration="1h0m0s"
I0923 18:39:17.529754       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 18:44:23.375727       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 18:49:30.782395       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 18:54:36.764775       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 18:59:43.848300       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 19:04:49.877094       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 19:09:56.923540       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 19:15:02.889326       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 19:20:07.659069       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 19:25:14.614058       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 19:30:21.111216       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 19:35:28.023400       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 19:40:34.119291       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 19:45:40.298279       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 20:10:09.943846       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 20:15:16.484048       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
E0923 20:15:42.058599       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0923 20:15:42.272815       1 garbagecollector.go:828] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I0923 20:20:22.817734       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 20:25:28.205764       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 20:30:35.074414       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 20:35:41.408507       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0923 20:40:47.402490       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [580fd29f40c3] <==
E0924 14:54:18.015572       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0924 14:54:18.076894       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0924 14:54:18.218681       1 server_linux.go:66] "Using iptables proxy"
I0924 14:54:18.926667       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0924 14:54:18.926970       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0924 14:54:19.561004       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0924 14:54:19.561276       1 server_linux.go:169] "Using iptables Proxier"
I0924 14:54:19.582693       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0924 14:54:19.613755       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0924 14:54:19.644650       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0924 14:54:19.647283       1 server.go:483] "Version info" version="v1.31.0"
I0924 14:54:19.647575       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0924 14:54:19.666117       1 config.go:104] "Starting endpoint slice config controller"
I0924 14:54:19.666165       1 config.go:326] "Starting node config controller"
I0924 14:54:19.666129       1 config.go:197] "Starting service config controller"
I0924 14:54:19.670469       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0924 14:54:19.670463       1 shared_informer.go:313] Waiting for caches to sync for service config
I0924 14:54:19.670504       1 shared_informer.go:313] Waiting for caches to sync for node config
I0924 14:54:19.771151       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0924 14:54:19.771497       1 shared_informer.go:320] Caches are synced for service config
I0924 14:54:19.771929       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [c969465cc83a] <==
E0923 17:38:16.533760       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0923 17:38:16.541763       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0923 17:38:16.565297       1 server_linux.go:66] "Using iptables proxy"
I0923 17:38:17.134293       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0923 17:38:17.134762       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0923 17:38:17.248183       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0923 17:38:17.248303       1 server_linux.go:169] "Using iptables Proxier"
I0923 17:38:17.251467       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0923 17:38:17.261210       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0923 17:38:17.268712       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0923 17:38:17.269802       1 server.go:483] "Version info" version="v1.31.0"
I0923 17:38:17.269871       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0923 17:38:17.275642       1 config.go:197] "Starting service config controller"
I0923 17:38:17.275721       1 config.go:104] "Starting endpoint slice config controller"
I0923 17:38:17.275885       1 config.go:326] "Starting node config controller"
I0923 17:38:17.277595       1 shared_informer.go:313] Waiting for caches to sync for service config
I0923 17:38:17.277593       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0923 17:38:17.277596       1 shared_informer.go:313] Waiting for caches to sync for node config
I0923 17:38:17.378552       1 shared_informer.go:320] Caches are synced for service config
I0923 17:38:17.378583       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0923 17:38:17.378552       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [0ec7b547661b] <==
I0924 14:54:03.890624       1 serving.go:386] Generated self-signed cert in-memory
W0924 14:54:12.177265       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0924 14:54:12.177342       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0924 14:54:12.177374       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W0924 14:54:12.177399       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0924 14:54:12.578233       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I0924 14:54:12.578290       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0924 14:54:12.582248       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0924 14:54:12.582434       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0924 14:54:12.592547       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0924 14:54:12.592732       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0924 14:54:12.892946       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [7a2e01a34d04] <==
I0923 17:37:55.558296       1 serving.go:386] Generated self-signed cert in-memory
I0923 17:38:04.147382       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I0923 17:38:04.147659       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0923 17:38:04.363989       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0923 17:38:04.364314       1 requestheader_controller.go:172] Starting RequestHeaderAuthRequestController
I0923 17:38:04.365971       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0923 17:38:04.366417       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0923 17:38:04.442242       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0923 17:38:04.454296       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0923 17:38:04.457313       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0923 17:38:04.459772       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I0923 17:38:04.763742       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0923 17:38:04.841531       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
I0923 17:38:04.861428       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0923 20:44:24.210530       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0923 20:44:24.210587       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
E0923 20:44:24.210668       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Sep 24 17:12:34 minikube kubelet[1626]: E0924 17:12:34.411739    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-vwqvh" podUID="373f0d80-a64b-4d68-a38d-dc42feb40498"
Sep 24 17:12:35 minikube kubelet[1626]: E0924 17:12:35.409847    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-8vw85" podUID="e2e7ae3c-024f-47fb-b1bd-30a462676d63"
Sep 24 17:12:41 minikube kubelet[1626]: E0924 17:12:41.400273    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-rvvt7" podUID="7bf69582-b7ab-4c41-a95f-daea7003a462"
Sep 24 17:12:45 minikube kubelet[1626]: E0924 17:12:45.404704    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-vwqvh" podUID="373f0d80-a64b-4d68-a38d-dc42feb40498"
Sep 24 17:12:47 minikube kubelet[1626]: E0924 17:12:47.404620    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-lws2g" podUID="c1bcbc0f-5b37-4565-a247-8154a93dc28c"
Sep 24 17:12:49 minikube kubelet[1626]: E0924 17:12:49.406130    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-8vw85" podUID="e2e7ae3c-024f-47fb-b1bd-30a462676d63"
Sep 24 17:12:53 minikube kubelet[1626]: E0924 17:12:53.402481    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-rvvt7" podUID="7bf69582-b7ab-4c41-a95f-daea7003a462"
Sep 24 17:12:58 minikube kubelet[1626]: E0924 17:12:58.404112    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-lws2g" podUID="c1bcbc0f-5b37-4565-a247-8154a93dc28c"
Sep 24 17:12:58 minikube kubelet[1626]: E0924 17:12:58.404153    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-vwqvh" podUID="373f0d80-a64b-4d68-a38d-dc42feb40498"
Sep 24 17:13:03 minikube kubelet[1626]: E0924 17:13:03.405317    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-8vw85" podUID="e2e7ae3c-024f-47fb-b1bd-30a462676d63"
Sep 24 17:13:04 minikube kubelet[1626]: E0924 17:13:04.403991    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-rvvt7" podUID="7bf69582-b7ab-4c41-a95f-daea7003a462"
Sep 24 17:13:10 minikube kubelet[1626]: E0924 17:13:10.397216    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-vwqvh" podUID="373f0d80-a64b-4d68-a38d-dc42feb40498"
Sep 24 17:13:13 minikube kubelet[1626]: E0924 17:13:13.404232    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-lws2g" podUID="c1bcbc0f-5b37-4565-a247-8154a93dc28c"
Sep 24 17:13:16 minikube kubelet[1626]: E0924 17:13:16.396314    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-8vw85" podUID="e2e7ae3c-024f-47fb-b1bd-30a462676d63"
Sep 24 17:13:18 minikube kubelet[1626]: E0924 17:13:18.401469    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-rvvt7" podUID="7bf69582-b7ab-4c41-a95f-daea7003a462"
Sep 24 17:13:22 minikube kubelet[1626]: E0924 17:13:22.402350    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-vwqvh" podUID="373f0d80-a64b-4d68-a38d-dc42feb40498"
Sep 24 17:13:27 minikube kubelet[1626]: E0924 17:13:27.400863    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-8vw85" podUID="e2e7ae3c-024f-47fb-b1bd-30a462676d63"
Sep 24 17:13:28 minikube kubelet[1626]: E0924 17:13:28.398230    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-lws2g" podUID="c1bcbc0f-5b37-4565-a247-8154a93dc28c"
Sep 24 17:13:33 minikube kubelet[1626]: E0924 17:13:33.402630    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-rvvt7" podUID="7bf69582-b7ab-4c41-a95f-daea7003a462"
Sep 24 17:13:35 minikube kubelet[1626]: E0924 17:13:35.403477    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-vwqvh" podUID="373f0d80-a64b-4d68-a38d-dc42feb40498"
Sep 24 17:13:40 minikube kubelet[1626]: E0924 17:13:40.397939    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-lws2g" podUID="c1bcbc0f-5b37-4565-a247-8154a93dc28c"
Sep 24 17:13:43 minikube kubelet[1626]: E0924 17:13:43.399538    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-8vw85" podUID="e2e7ae3c-024f-47fb-b1bd-30a462676d63"
Sep 24 17:13:45 minikube kubelet[1626]: E0924 17:13:45.395664    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-rvvt7" podUID="7bf69582-b7ab-4c41-a95f-daea7003a462"
Sep 24 17:13:49 minikube kubelet[1626]: E0924 17:13:49.398746    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-vwqvh" podUID="373f0d80-a64b-4d68-a38d-dc42feb40498"
Sep 24 17:13:52 minikube kubelet[1626]: E0924 17:13:52.397028    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-lws2g" podUID="c1bcbc0f-5b37-4565-a247-8154a93dc28c"
Sep 24 17:13:54 minikube kubelet[1626]: E0924 17:13:54.398303    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-8vw85" podUID="e2e7ae3c-024f-47fb-b1bd-30a462676d63"
Sep 24 17:13:56 minikube kubelet[1626]: E0924 17:13:56.397142    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-rvvt7" podUID="7bf69582-b7ab-4c41-a95f-daea7003a462"
Sep 24 17:14:03 minikube kubelet[1626]: E0924 17:14:03.399225    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-lws2g" podUID="c1bcbc0f-5b37-4565-a247-8154a93dc28c"
Sep 24 17:14:04 minikube kubelet[1626]: E0924 17:14:04.408904    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-vwqvh" podUID="373f0d80-a64b-4d68-a38d-dc42feb40498"
Sep 24 17:14:05 minikube kubelet[1626]: E0924 17:14:05.394811    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-8vw85" podUID="e2e7ae3c-024f-47fb-b1bd-30a462676d63"
Sep 24 17:14:10 minikube kubelet[1626]: E0924 17:14:10.395624    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-rvvt7" podUID="7bf69582-b7ab-4c41-a95f-daea7003a462"
Sep 24 17:14:16 minikube kubelet[1626]: E0924 17:14:16.389330    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-lws2g" podUID="c1bcbc0f-5b37-4565-a247-8154a93dc28c"
Sep 24 17:14:16 minikube kubelet[1626]: E0924 17:14:16.389467    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-8vw85" podUID="e2e7ae3c-024f-47fb-b1bd-30a462676d63"
Sep 24 17:14:17 minikube kubelet[1626]: E0924 17:14:17.395525    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-vwqvh" podUID="373f0d80-a64b-4d68-a38d-dc42feb40498"
Sep 24 17:14:24 minikube kubelet[1626]: E0924 17:14:24.396484    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-rvvt7" podUID="7bf69582-b7ab-4c41-a95f-daea7003a462"
Sep 24 17:14:29 minikube kubelet[1626]: E0924 17:14:29.395833    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-lws2g" podUID="c1bcbc0f-5b37-4565-a247-8154a93dc28c"
Sep 24 17:14:30 minikube kubelet[1626]: E0924 17:14:30.394769    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-vwqvh" podUID="373f0d80-a64b-4d68-a38d-dc42feb40498"
Sep 24 17:14:30 minikube kubelet[1626]: E0924 17:14:30.394769    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-8vw85" podUID="e2e7ae3c-024f-47fb-b1bd-30a462676d63"
Sep 24 17:14:36 minikube kubelet[1626]: E0924 17:14:36.388595    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-rvvt7" podUID="7bf69582-b7ab-4c41-a95f-daea7003a462"
Sep 24 17:14:40 minikube kubelet[1626]: E0924 17:14:40.388638    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-lws2g" podUID="c1bcbc0f-5b37-4565-a247-8154a93dc28c"
Sep 24 17:14:41 minikube kubelet[1626]: E0924 17:14:41.386069    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-8vw85" podUID="e2e7ae3c-024f-47fb-b1bd-30a462676d63"
Sep 24 17:14:45 minikube kubelet[1626]: E0924 17:14:45.389109    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-vwqvh" podUID="373f0d80-a64b-4d68-a38d-dc42feb40498"
Sep 24 17:14:47 minikube kubelet[1626]: E0924 17:14:47.390092    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-rvvt7" podUID="7bf69582-b7ab-4c41-a95f-daea7003a462"
Sep 24 17:14:51 minikube kubelet[1626]: E0924 17:14:51.384903    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-lws2g" podUID="c1bcbc0f-5b37-4565-a247-8154a93dc28c"
Sep 24 17:14:55 minikube kubelet[1626]: E0924 17:14:55.391010    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-8vw85" podUID="e2e7ae3c-024f-47fb-b1bd-30a462676d63"
Sep 24 17:14:57 minikube kubelet[1626]: E0924 17:14:57.388951    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-vwqvh" podUID="373f0d80-a64b-4d68-a38d-dc42feb40498"
Sep 24 17:15:01 minikube kubelet[1626]: E0924 17:15:01.389197    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-rvvt7" podUID="7bf69582-b7ab-4c41-a95f-daea7003a462"
Sep 24 17:15:04 minikube kubelet[1626]: E0924 17:15:04.389322    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-lws2g" podUID="c1bcbc0f-5b37-4565-a247-8154a93dc28c"
Sep 24 17:15:07 minikube kubelet[1626]: E0924 17:15:07.386212    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-8vw85" podUID="e2e7ae3c-024f-47fb-b1bd-30a462676d63"
Sep 24 17:15:09 minikube kubelet[1626]: E0924 17:15:09.385778    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-vwqvh" podUID="373f0d80-a64b-4d68-a38d-dc42feb40498"
Sep 24 17:15:12 minikube kubelet[1626]: E0924 17:15:12.384541    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-rvvt7" podUID="7bf69582-b7ab-4c41-a95f-daea7003a462"
Sep 24 17:15:15 minikube kubelet[1626]: E0924 17:15:15.385962    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-lws2g" podUID="c1bcbc0f-5b37-4565-a247-8154a93dc28c"
Sep 24 17:15:18 minikube kubelet[1626]: E0924 17:15:18.384565    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-8vw85" podUID="e2e7ae3c-024f-47fb-b1bd-30a462676d63"
Sep 24 17:15:21 minikube kubelet[1626]: E0924 17:15:21.389519    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-vwqvh" podUID="373f0d80-a64b-4d68-a38d-dc42feb40498"
Sep 24 17:15:26 minikube kubelet[1626]: E0924 17:15:26.466360    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-rvvt7" podUID="7bf69582-b7ab-4c41-a95f-daea7003a462"
Sep 24 17:15:29 minikube kubelet[1626]: E0924 17:15:29.386093    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-lws2g" podUID="c1bcbc0f-5b37-4565-a247-8154a93dc28c"
Sep 24 17:15:32 minikube kubelet[1626]: E0924 17:15:32.383385    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-8vw85" podUID="e2e7ae3c-024f-47fb-b1bd-30a462676d63"
Sep 24 17:15:36 minikube kubelet[1626]: E0924 17:15:36.380436    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-vwqvh" podUID="373f0d80-a64b-4d68-a38d-dc42feb40498"
Sep 24 17:15:38 minikube kubelet[1626]: E0924 17:15:38.380285    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-rvvt7" podUID="7bf69582-b7ab-4c41-a95f-daea7003a462"
Sep 24 17:15:43 minikube kubelet[1626]: E0924 17:15:43.374157    1626 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"user-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"user-newapp:2.0\\\"\"" pod="user-app/user-svc-deployment-5f8649bf9f-lws2g" podUID="c1bcbc0f-5b37-4565-a247-8154a93dc28c"


==> kubernetes-dashboard [edcdabd35646] <==
2024/09/24 17:14:36 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:36 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:36 [2024-09-24T17:14:36Z] Outcoming response to 127.0.0.1 with 200 status code
2024/09/24 17:14:36 [2024-09-24T17:14:36Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/09/24 17:14:36 Getting list of namespaces
2024/09/24 17:14:36 [2024-09-24T17:14:36Z] Outcoming response to 127.0.0.1 with 200 status code
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Incoming HTTP/1.1 GET /api/v1/service/user-app/userapp-service request from 127.0.0.1: 
2024/09/24 17:14:41 Getting details of userapp-service service in user-app namespace
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Incoming HTTP/1.1 GET /api/v1/service/user-app/userapp-service/pod?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Incoming HTTP/1.1 GET /api/v1/service/user-app/userapp-service/ingress?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Incoming HTTP/1.1 GET /api/v1/service/user-app/userapp-service/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2024/09/24 17:14:41 Found 4 endpoints related to userapp-service service in user-app namespace
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Outcoming response to 127.0.0.1 with 200 status code
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Outcoming response to 127.0.0.1 with 200 status code
2024/09/24 17:14:41 Found 1 events related to userapp-service service in user-app namespace
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Outcoming response to 127.0.0.1 with 200 status code
2024/09/24 17:14:41 received 0 resources from sidecar instead of 4
2024/09/24 17:14:41 received 0 resources from sidecar instead of 4
2024/09/24 17:14:41 Getting pod metrics
2024/09/24 17:14:41 received 0 resources from sidecar instead of 4
2024/09/24 17:14:41 received 0 resources from sidecar instead of 4
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Outcoming response to 127.0.0.1 with 200 status code
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/09/24 17:14:41 Getting list of namespaces
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Outcoming response to 127.0.0.1 with 200 status code
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Incoming HTTP/1.1 GET /api/v1/service/user-app/userapp-service request from 127.0.0.1: 
2024/09/24 17:14:41 Getting details of userapp-service service in user-app namespace
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/09/24 17:14:41 Getting list of namespaces
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Outcoming response to 127.0.0.1 with 200 status code
2024/09/24 17:14:41 Found 4 endpoints related to userapp-service service in user-app namespace
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Outcoming response to 127.0.0.1 with 200 status code
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Incoming HTTP/1.1 GET /api/v1/service/user-app/userapp-service/pod?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Incoming HTTP/1.1 GET /api/v1/service/user-app/userapp-service/ingress?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Outcoming response to 127.0.0.1 with 200 status code
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Incoming HTTP/1.1 GET /api/v1/service/user-app/userapp-service/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2024/09/24 17:14:41 Found 1 events related to userapp-service service in user-app namespace
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Outcoming response to 127.0.0.1 with 200 status code
2024/09/24 17:14:41 received 0 resources from sidecar instead of 4
2024/09/24 17:14:41 received 0 resources from sidecar instead of 4
2024/09/24 17:14:41 Getting pod metrics
2024/09/24 17:14:41 received 0 resources from sidecar instead of 4
2024/09/24 17:14:41 received 0 resources from sidecar instead of 4
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 Skipping metric because of error: Metric label not set.
2024/09/24 17:14:41 [2024-09-24T17:14:41Z] Outcoming response to 127.0.0.1 with 200 status code


==> storage-provisioner [6ef6a3e38d39] <==
I0924 14:54:53.510751       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0924 14:54:53.616491       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0924 14:54:53.619173       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0924 14:55:11.093004       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0924 14:55:11.093737       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_6f896b91-d79f-4110-847a-2f1976fcf569!
I0924 14:55:11.094056       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"dad3c29b-7d46-441c-9f3f-fa629437db43", APIVersion:"v1", ResourceVersion:"8827", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_6f896b91-d79f-4110-847a-2f1976fcf569 became leader
I0924 14:55:11.197430       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_6f896b91-d79f-4110-847a-2f1976fcf569!


==> storage-provisioner [b14f164675ee] <==
I0924 14:54:17.723496       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0924 14:54:38.827369       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

